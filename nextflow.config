// ---------------------------------------------------------------------------------------------- //
// Setup default parameters
// ---------------------------------------------------------------------------------------------- //
params.sra = "NULL"
params.nucl = "rna"
params.output = "results/"
params.threads = "2"
params.memory = "4"
params.tempdir = "/tmp/"
params.ncbi_apikey = "2aa9bc6729046b7e50e3ed0c77dc33def809"

// Diamond settings
params.diamondDB = "/n/data1/hms/mbib/nibert/austin/diamond/nr.dmnd"
params.blocksize = "10"

// Kraken
params.krakenDB = "/n/data1/hms/mbib/nibert/austin/tools/kraken2/kraken_viral"
// ---------------------------------------------------------------------------------------------- //

//----------------------------------------------------------------------------//
// Set up two profiles: running the pipeline locally vs. remotely
//----------------------------------------------------------------------------//
profiles {

    // Set up a profile for running the pipeline locally [the default]
    standard {

        process {
            executor = "local"
            cache = "lenient"
            conda = "$baseDir/resources/conda.yml"
            memory = params.memory + "GB"
            cpus = params.threads
        }

        report {
          enabled = true
          file = "$params.output/reports/raven-nf.${params.sra.split(',')[0]}-${params.sra.split(',')[-1]}.report.html"
        }

        timeline {
          enabled = true
          file = "$params.output/reports/raven-nf.${params.sra.split(',')[0]}-${params.sra.split(',')[-1]}.timeline.html"
        }

        trace {
          enabled = true
          file = "$params.output/reports/raven-nf.${params.sra.split(',')[0]}-${params.sra.split(',')[-1]}.trace.txt"
        }

        conda {
          // Where to save the conda environment so it doesn"t need to be re-generated.
          cacheDir = "$baseDir/resources/cache/"
          createTimeout = "1 h"
        }
    }

    // Set up a profile for running on the cluster & specify the SLURM resources to be used for each job
    cluster {

        // Set O2 user-specific temporary directory
        params.tempdir = "/n/scratch3/users/a/am704/nextflow/tmp/"

        process {

          // Global settings
          executor = "slurm"
          queue = "short"
          cache = "lenient"
          conda = "$baseDir/resources/conda.yml"

          // Error handling
          errorStrategy = "retry"
          maxRetries = 3

          // Set per-process resources
          withName: "log_inputs" {
              time    = { 1.m * task.attempt }
              memory  = { 1.GB * task.attempt }
              cpus    = 1
              }

          withName: "parse_sra_ids" {
              time    = { 1.m * task.attempt }
              memory  = { 1.GB * task.attempt }
              cpus    = 1
              }

          withName: "download_sra_files" {
              time    = { 30.m * task.attempt }
              memory  = { 16.GB * task.attempt }
              cpus    = 4
              }

          withName: "combine_reads" {
              time    = { 30.m * task.attempt }
              memory  = { 16.GB * task.attempt }
              cpus    = 1
              }

          withName: "de_novo_assembly" {
              time    = { 2.h * task.attempt }
              memory  = { 80.GB * task.attempt }
              cpus    = 8
              }

          withName: "refine_contigs" {
              time    = { 3.h * task.attempt }
              memory  = { 32.GB * task.attempt }
              cpus    = 4
              }

          withName: "coverage" {
              time    = { 1.h * task.attempt }
              memory  = { 16.GB * task.attempt }
              cpus    = 1
              }

          withName: "classify_contigs" {
              time    = { 6.h * task.attempt }
              memory  = { 120.GB * task.attempt }
              cpus    = 4
              }

          withName: "taxonomy" {
              time    = { 1.h * task.attempt }
              memory  = { 16.GB * task.attempt }
              cpus    = 1
              }

          withName: "classify_reads" {
              time    = { 1.h * task.attempt }
              memory  = { 16.GB * task.attempt }
              cpus    = 4
              }

          withName: "visualize_reads" {
              time    = { 1.h * task.attempt }
              memory  = { 8.GB * task.attempt }
              cpus    = 1
              }

          withName: "identify_viruses" {
              time    = { 15.m * task.attempt }
              memory  = { 4.GB * task.attempt }
              cpus    = 1
              }

          withName: "print_results" {
              time    = { 5.m * task.attempt }
              memory  = { 2.GB * task.attempt }
              cpus    = 1
              }

        }

        executor {
          // Max jobs that Nextflow can submit at once (basically set to unlimited)
          queueSize = 10000
        }

        report {
          enabled = true
          file = "$params.output/reports/raven-nf.${params.sra.split(',')[0]}-${params.sra.split(',')[-1]}.report.html"
        }

        timeline {
          enabled = true
          file = "$params.output/reports/raven-nf.${params.sra.split(',')[0]}-${params.sra.split(',')[-1]}.timeline.html"
        }

        trace {
          enabled = true
          file = "$params.output/reports/raven-nf.${params.sra.split(',')[0]}-${params.sra.split(',')[-1]}.trace.txt"
        }

        conda {
          // Where to save the conda environment so it doesn"t need to be re-generated.
          cacheDir = "$baseDir/resources/cache"
        }
    }
}
